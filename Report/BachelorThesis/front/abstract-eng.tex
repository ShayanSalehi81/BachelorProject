
% -------------------------------------------------------
%  English Abstract
% -------------------------------------------------------

\begin{latin}

\begin{center}
\textbf{Abstract}
\end{center}
\baselineskip=.8\baselineskip

ŸèThis work examines the capability of large language models (LLMs) to messure the importance of Persian news, evaluating their learning ability from content, reasoning skills, and overall cognitive capacities. Initially, annotated datasets were collected from various domains, including sports, politics, social issues, medicine, and culture, to develop an evaluation framework for LLMs. Within this framework, various existing models were analyzed and assessed under different scenarios and conditions to evaluate their analytical performance in both Persian and English. The findings indicate that prompts incorporating Chain-of-Thoughts and Tree-of-Thoughts significantly improve the models' performance. Additionally, the Symbol Tuning method enhances sensitivity to the input queries and their content.


\bigskip\noindent\textbf{Keywords}:
Large Language Models, Natural Language Processing, Machine Learning, News Importance Detection

\end{latin}
