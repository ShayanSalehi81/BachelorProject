{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_labels(x):\n",
    "    if type(x) is str:\n",
    "        x = x.replace('\\u200c', '')\n",
    "    if x == 'important' or x == '\\'important\\'' or x == '1' or x == '\\'1\\'' or x == '58' or x == 58 or x == '\\'58\\'':\n",
    "        return 1\n",
    "    elif x == 'not important' or x == '\\'not important\\'' or x == '0' or x == '\\'0\\'' or x == '47' or x == 47 or x == '\\'47\\'':\n",
    "        return 0\n",
    "    if math.isnan(x):\n",
    "        return x\n",
    "    return int(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "aya_df = pd.read_csv('../Datasets/Symbol_tuning_Gemma_Results.csv')\n",
    "columns_to_evaluate = ['predicted_k_0', 'predicted_k_1', 'predicted_k_5', 'predicted_k_20', 'predicted_k_50']\n",
    "# columns_to_evaluate = ['predicted_k_20']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aya_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "aya_df['real_tag'] = aya_df['real_tag'].apply(map_labels)\n",
    "for column in columns_to_evaluate:\n",
    "    aya_df[column] = aya_df[column].apply(map_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aya_df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = aya_df['real_tag'].dropna()\n",
    "true_labels_first_400 = aya_df['real_tag'].dropna().iloc[:401]\n",
    "\n",
    "# num_true1 = (true_labels_first_400 == 1).sum()\n",
    "# num_true0 = (true_labels_first_400 == 0).sum()\n",
    "\n",
    "num_true1 = (true_labels == 1).sum()\n",
    "num_true0 = (true_labels == 0).sum()\n",
    "\n",
    "print(f\"  Number of true '1' labels: {num_true1}\")\n",
    "print(f\"  Number of true '0' labels: {num_true0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in columns_to_evaluate:\n",
    "\n",
    "    pred_labels = aya_df[column].dropna()\n",
    "\n",
    "    common_indices = true_labels.index.intersection(pred_labels.index)\n",
    "\n",
    "    true_labels = true_labels.loc[common_indices]\n",
    "    pred_labels = pred_labels.loc[common_indices]\n",
    "\n",
    "    for index, pred_label in enumerate(pred_labels):\n",
    "        if pred_label != 0 and pred_label != 1:\n",
    "            print(index)\n",
    "            print(pred_label)\n",
    "            pred_labels[index] = 1\n",
    "\n",
    "    for index, true_label in enumerate(true_labels):\n",
    "        if true_label != 0 and true_label != 1:\n",
    "            print(index)\n",
    "            print(true_label)\n",
    "\n",
    "    accuracy = accuracy_score(true_labels, pred_labels)\n",
    "    precision = precision_score(true_labels, pred_labels)\n",
    "    recall = recall_score(true_labels, pred_labels)\n",
    "    f1 = f1_score(true_labels, pred_labels)\n",
    "\n",
    "    num_58 = (aya_df[column] == 1).sum()\n",
    "    num_47 = (aya_df[column] == 0).sum()\n",
    "\n",
    "    print(f\"Metrics for column {column}:\")\n",
    "    print(f\"  Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"  Precision: {precision:.2f}\")\n",
    "    print(f\"  Recall: {recall:.2f}\")\n",
    "    print(f\"  F1 Score: {f1:.2f}\")\n",
    "    print(f\"  Number of '58' labels: {num_58}\")\n",
    "    print(f\"  Number of '47' labels: {num_47}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in columns_to_evaluate:\n",
    "\n",
    "    pred_labels = aya_df[column].dropna()\n",
    "    aya_df[column] = aya_df[column].apply(map_labels)\n",
    "\n",
    "    common_indices = true_labels.index.intersection(pred_labels.index)\n",
    "    true_labels_common = true_labels.loc[common_indices]\n",
    "    pred_labels_common = pred_labels.loc[common_indices]\n",
    "\n",
    "    print(f\"Metrics for column {column}:\")\n",
    "\n",
    "    report = classification_report(true_labels_common, pred_labels_common, target_names=['0', '1'])\n",
    "    print(report)\n",
    "\n",
    "    num_1 = (aya_df[column] == 1).sum()\n",
    "    num_0 = (aya_df[column] == 0).sum()\n",
    "\n",
    "    print(f\"  Number of '1' labels: {num_58}\")\n",
    "    print(f\"  Number of '0' labels: {num_47}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *kfold Measurment*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "macro_avg_precision_scores = []\n",
    "macro_avg_recall_scores = []\n",
    "macro_avg_f1_scores = []\n",
    "macro_avg_accuracy_scores = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold, (train_index, test_index) in enumerate(kf.split(aya_df)):\n",
    "    print(f\"\\nFold {fold + 1}/{n_splits}\")\n",
    "\n",
    "    train_df = aya_df.iloc[train_index]\n",
    "    test_df = aya_df.iloc[test_index]\n",
    "\n",
    "    train_df['real_tag'] = train_df['real_tag'].apply(map_labels)\n",
    "    test_df['real_tag'] = test_df['real_tag'].apply(map_labels)\n",
    "\n",
    "    for column in columns_to_evaluate:\n",
    "        train_df[column] = train_df[column].apply(map_labels)\n",
    "        test_df[column] = test_df[column].apply(map_labels)\n",
    "\n",
    "        true_labels_test = test_df['real_tag'].dropna()\n",
    "        pred_labels_test = test_df[column].dropna()\n",
    "\n",
    "        common_indices_test = true_labels_test.index.intersection(pred_labels_test.index)\n",
    "        \n",
    "        true_labels_test = true_labels_test.loc[common_indices_test]\n",
    "        pred_labels_test = pred_labels_test.loc[common_indices_test]\n",
    "\n",
    "        accuracy = accuracy_score(true_labels_test, pred_labels_test)\n",
    "        report_dict = classification_report(true_labels_test, pred_labels_test, target_names=['0', '1'], output_dict=True)\n",
    "\n",
    "        macro_avg_precision = report_dict['macro avg']['precision']\n",
    "        macro_avg_recall = report_dict['macro avg']['recall']\n",
    "        macro_avg_f1 = report_dict['macro avg']['f1-score']\n",
    "\n",
    "        macro_avg_accuracy_scores.append(accuracy)\n",
    "        macro_avg_precision_scores.append(macro_avg_precision)\n",
    "        macro_avg_recall_scores.append(macro_avg_recall)\n",
    "        macro_avg_f1_scores.append(macro_avg_f1)\n",
    "\n",
    "        print(f\"\\nMetrics for column {column} in fold {fold + 1}:\")\n",
    "        report = classification_report(true_labels_test, pred_labels_test, target_names=['0', '1'])\n",
    "        print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== K-Fold Cross Validation Results (Macro Avg) ===\")\n",
    "print(f\"Accuracy: Mean={np.mean(macro_avg_accuracy_scores):.4f}, Variance={np.var(macro_avg_accuracy_scores):.4f}\")\n",
    "print(f\"Precision (Macro Avg): Mean={np.mean(macro_avg_precision_scores):.4f}, Variance={np.var(macro_avg_precision_scores):.4f}\")\n",
    "print(f\"Recall (Macro Avg): Mean={np.mean(macro_avg_recall_scores):.4f}, Variance={np.var(macro_avg_recall_scores):.4f}\")\n",
    "print(f\"F1 Score (Macro Avg): Mean={np.mean(macro_avg_f1_scores):.4f}, Variance={np.var(macro_avg_f1_scores):.4f}\")\n",
    "\n",
    "\n",
    "print(f\"\\nAccuracy: Range={np.max(macro_avg_accuracy_scores) - np.min(macro_avg_accuracy_scores):.4f}\")\n",
    "print(f\"Precision (Macro Avg): Range={np.max(macro_avg_precision_scores) - np.min(macro_avg_precision_scores):.4f}\")\n",
    "print(f\"Recall (Macro Avg): Range={np.max(macro_avg_recall_scores) - np.min(macro_avg_recall_scores):.4f}\")\n",
    "print(f\"F1 Score (Macro Avg): Range={np.max(macro_avg_f1_scores) - np.min(macro_avg_f1_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *F1 Calculation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import arabic_reshaper\n",
    "from bidi.algorithm import get_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_numbers(column):\n",
    "    persian_to_english = {\n",
    "        '۰': '0', '۱': '1', '۲': '2', '۳': '3', '۴': '4', \n",
    "        '۵': '5', '۶': '6', '۷': '7', '۸': '8', '۹': '9'\n",
    "    }\n",
    "    column = column.astype(str)\n",
    "    for persian_num, eng_num in persian_to_english.items():\n",
    "        column = column.str.replace(persian_num, eng_num)\n",
    "    return pd.to_numeric(column, errors='coerce').fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_persian_text(text):\n",
    "    reshaped_text = arabic_reshaper.reshape(text)\n",
    "    return get_display(reshaped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f1_scores(df, col_true, col_pred=None, category_col='category'): \n",
    "    if col_pred is None:\n",
    "        col_pred = 'tag'\n",
    "    \n",
    "    y_true = normalize_numbers(df[col_true])\n",
    "    y_pred = normalize_numbers(df[col_pred])\n",
    "    \n",
    "    f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "    f1_micro = f1_score(y_true, y_pred, average='micro')\n",
    "    precision = precision_score(y_true, y_pred, average='macro')\n",
    "    recall = recall_score(y_true, y_pred, average='macro')\n",
    "    \n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    conf_matrix_normalized = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix_normalized, annot=True, fmt='.2%', cmap='Blues')\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.show()\n",
    "    \n",
    "    unique_categories = df[category_col].unique()\n",
    "    category_f1_scores = {}\n",
    "    \n",
    "    for category in unique_categories:\n",
    "        category_mask = (df[category_col] == category)\n",
    "        y_true_cat = y_true[category_mask]\n",
    "        y_pred_cat = y_pred[category_mask]\n",
    "        \n",
    "        f1_macro_cat = f1_score(y_true_cat, y_pred_cat, average='macro')\n",
    "        f1_micro_cat = f1_score(y_true_cat, y_pred_cat, average='micro')\n",
    "        precision_cat = precision_score(y_true_cat, y_pred_cat, average='macro')\n",
    "        recall_cat = recall_score(y_true_cat, y_pred_cat, average='macro')\n",
    "        \n",
    "        category_f1_scores[category] = {\n",
    "            'F1 Macro': round(f1_macro_cat, 4),\n",
    "            'F1 Micro': round(f1_micro_cat, 4),\n",
    "            'Precision': round(precision_cat, 4),\n",
    "            'Recall': round(recall_cat, 4)\n",
    "        }\n",
    "    \n",
    "    category_f1_scores['Overall'] = {\n",
    "        'F1 Macro': round(f1_macro, 4),\n",
    "        'F1 Micro': round(f1_micro, 4),\n",
    "        'Precision': round(precision, 4),\n",
    "        'Recall': round(recall, 4),\n",
    "    }\n",
    "    \n",
    "    category_f1_scores = {reshape_persian_text(cat): metrics for cat, metrics in category_f1_scores.items()}\n",
    "    \n",
    "    print(\"\\nCategory-wise F1 Scores:\")\n",
    "    for category, metrics in category_f1_scores.items():\n",
    "        print(f\"\\nCategory: {category}\")\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"{metric}: {value}\")\n",
    "    \n",
    "    metrics_df = pd.DataFrame.from_dict(category_f1_scores, orient='index')\n",
    "    \n",
    "    metrics_df.plot(kind='bar', figsize=(10, 6))\n",
    "    plt.title(reshape_persian_text(\"Category-wise and Overall F1, Precision, and Recall\"))\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.xlabel(reshape_persian_text(\"Category\"))\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.axis('off')\n",
    "    table = plt.table(cellText=metrics_df.values,\n",
    "                      colLabels=metrics_df.columns,\n",
    "                      rowLabels=metrics_df.index,\n",
    "                      loc='center',\n",
    "                      cellLoc='center',\n",
    "                      colWidths=[0.2] * len(metrics_df.columns))\n",
    "    \n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1.2, 1.2) \n",
    "    plt.title(reshape_persian_text(\"F1, Precision, and Recall Summary Table\"), pad=20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_f1_scores(\n",
    "    df=aya_df,\n",
    "    col_true='real_tag',\n",
    "    col_pred='predicted_k_20',\n",
    "    category_col='category'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
